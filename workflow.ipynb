{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "                    \n",
    "from dask.distributed import LocalCluster, SSHCluster \n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, MacroPipeline\n",
    "from laserfarm.remote_utils import get_wdclient, get_info_remote, list_remote\n",
    "\n",
    "def last_modified(opts, remote_path):\n",
    "    info = get_info_remote(get_wdclient(opts), remote_path.as_posix())\n",
    "    format_ = '%a, %d %b %Y %H:%M:%S GMT'\n",
    "    return datetime.datetime.strptime(info['modified'], format_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-Pipeline Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Run-Specific Input\n",
    "\n",
    "Fill in the username/password for the SURF dCache. LAZ files updated since the last workflow run will be re-run through the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdav_login = input('WebDAV username: ')\n",
    "webdav_password = getpass.getpass('WebDAV password: ')\n",
    "last_run = datetime.datetime.strptime(input('Date last run (YYYY-MM-DD): '), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Connection to Remote Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_path_root = pathlib.Path('/pnfs/grid.sara.nl/data/projects.nl/eecolidar/01_Escience/')\n",
    "wd_opts = {\n",
    "    'webdav_hostname': 'https://webdav.grid.surfsara.nl:2880',\n",
    "    'webdav_login': webdav_login,\n",
    "    'webdav_password': webdav_password\n",
    "}\n",
    "assert get_wdclient(wd_opts).check(remote_path_root.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cluster\n",
    "\n",
    "Setup Dask cluster used for all the macro-pipeline calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_tmp = pathlib.Path('/tmp')\n",
    "cluster = LocalCluster(processes=True, \n",
    "                       n_workers=2, \n",
    "                       threads_per_worker=1, \n",
    "                       local_directory=local_tmp/'dask-worker-space')\n",
    "# nprocs_per_node = 2\n",
    "# cluster = SSHCluster(hosts=['172.17.0.2', \n",
    "#                             '172.17.0.2', \n",
    "#                             '172.17.0.3'], \n",
    "#                      connect_options={'known_hosts': None, \n",
    "#                                       'username': 'ubuntu', \n",
    "#                                       'client_keys': '/home/ubuntu/.ssh/id_rsa'},\n",
    "#                      worker_options={'nthreads': 1, \n",
    "#                                      'nprocs': nprocs_per_node,\n",
    "#                                      'local_directory': local_tmp/'dask-worker-space'}, \n",
    "#                      scheduler_options={'dashboard_address': '8787'})\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retiling\n",
    "\n",
    "The raw point-cloud files are downloaded and retiled to a regular grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dCache path to raw LAZ files \n",
    "remote_path_ahn = remote_path_root / 'test_pipeline/test_full/raw'\n",
    "\n",
    "# dCache path where to copy retiled PLY files\n",
    "remote_path_retiled = remote_path_ahn.parent / 'retiled'\n",
    "\n",
    "# details of the retiling schema\n",
    "grid = {\n",
    "    'min_x': -113107.81,\n",
    "    'max_x': 398892.19,\n",
    "    'min_y': 214783.87,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 512\n",
    "}\n",
    "\n",
    "# determine which LAZ files have been updated since the last run \n",
    "laz_files = [f for f in list_remote(get_wdclient(wd_opts), remote_path_ahn.as_posix())\n",
    "             if f.lower().endswith('.laz') and last_modified(wd_opts, remote_path_ahn/f) > last_run]\n",
    "print('Retrieve and retile: {} LAZ files'.format(len(laz_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input dictionary to configure the retiling pipeline\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix()},\n",
    "    'pullremote': remote_path_ahn.as_posix(),\n",
    "    'set_grid': grid,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {},\n",
    "    'pushremote': remote_path_retiled.as_posix(),\n",
    "    'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('retiling.json', 'w') as f:\n",
    "    json.dump(retiling_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [Retiler(file).config(retiling_input).setup_webdav_client(wd_opts) for file in laz_files]\n",
    "macro.set_labels([os.path.splitext(file)[0] for file in laz_files])\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run! \n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='retiling.out')\n",
    "assert not macro.get_failed_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Features computed for the retiled point-cloud data are assigned to a regular 'target' grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target mesh size & list of features\n",
    "tile_mesh_size = 10.\n",
    "features = ['perc_95_normalized_height', 'pulse_penetration_ratio', 'entropy_normalized_height', 'point_density']\n",
    "\n",
    "# dCache path where to copy the feature-enriched target data\n",
    "remote_path_targets = remote_path_ahn.parent / 'targets'\n",
    "\n",
    "# determine which tiles have been updated since last run, and extract tile index numbers\n",
    "tiles = [t.strip('/') for t in list_remote(get_wdclient(wd_opts), remote_path_retiled.as_posix())\n",
    "         if fnmatch.fnmatch(t, 'tile_*_*/') and last_modified(wd_opts, remote_path_retiled/t) > last_run]\n",
    "tile_indices = [[int(el) for el in tile.split('_')[1:]] for tile in tiles]\n",
    "print('Retrieve and process: {} tiles'.format(len(tile_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input dictionary to configure the feature extraction pipeline\n",
    "feature_extraction_input = {\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix()},\n",
    "    'pullremote': remote_path_retiled.as_posix(),\n",
    "    'load': {'attributes': ['raw_classification']},\n",
    "    'normalize': 1,\n",
    "    'apply_filter': {\n",
    "        'filter_type': 'select_equal', \n",
    "        'attribute': 'raw_classification',\n",
    "        'value': [1, 6]#ground surface (2), water (9), buildings (6), artificial objects (26), vegetation (?), and unclassified (1)\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : tile_mesh_size,\n",
    "        'validate' : True,\n",
    "        **grid\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': features,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': tile_mesh_size\n",
    "    },\n",
    "    'export_targets': {\n",
    "        'attributes': features,\n",
    "        'multi_band_files': False\n",
    "    },\n",
    "    'pushremote': remote_path_targets.as_posix(),\n",
    "#     'cleanlocalfs': {}\n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('feature_extraction.json', 'w') as f:\n",
    "    json.dump(feature_extraction_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [DataProcessing(t, tile_index=idx).config(feature_extraction_input).setup_webdav_client(wd_opts) \n",
    "               for t, idx in zip(tiles, tile_indices)]\n",
    "macro.set_labels(tiles)\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='feature_extraction.out')\n",
    "assert not macro.get_failed_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoTIFF Export\n",
    "\n",
    "Export the rasterized features from the target grid to GeoTIFF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dCache path where to copy the GeoTIFF files\n",
    "remote_path_geotiffs = remote_path_ahn.parent / 'geotiffs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input dictionary to configure the GeoTIFF export pipeline\n",
    "geotiff_export_input = {\n",
    "    'setup_local_fs': {'tmp_folder': local_tmp.as_posix()},\n",
    "    'pullremote': remote_path_targets.as_posix(),\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': {'xSub': 1, 'ySub': 1},\n",
    "    'create_subregion_geotiffs': {'output_handle': 'geotiff'},\n",
    "    'pushremote': remote_path_geotiffs.as_posix(),\n",
    "    'cleanlocalfs': {}   \n",
    "}\n",
    "\n",
    "# write input dictionary to JSON file\n",
    "with open('geotiff_export.json', 'w') as f:\n",
    "    json.dump(geotiff_export_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = MacroPipeline()\n",
    "\n",
    "# add pipeline list to macro-pipeline object and set the corresponding labels\n",
    "macro.tasks = [GeotiffWriter(input_dir=feature, bands=feature).config(geotiff_export_input).setup_webdav_client(wd_opts) \n",
    "               for feature in features]\n",
    "macro.set_labels(features)\n",
    "\n",
    "macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "macro.run()\n",
    "\n",
    "# save outcome results and check that no error occurred before continuing\n",
    "macro.print_outcome(to_file='geotiff_export.out')\n",
    "assert not macro.get_failed_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
