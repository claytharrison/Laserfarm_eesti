{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "from distutils import dir_util\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from laserfarm import Retiler, DataProcessing, GeotiffWriter, Classification\n",
    "from laserfarm import MacroPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-ecology LiDAR point-cloud processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Retrieval  and Cluster Setup\n",
    "\n",
    "Files produced by the pipeline will be saved in the `tmp_folder` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_folder = pathlib.Path('/var/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking whether the test data set is available locally, we otherwise retrieve it from the AHN3 repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_files = ['C_41CZ2.LAZ']\n",
    "\n",
    "file_paths = [tmp_folder/f for f in testdata_files]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if not file_path.is_file():\n",
    "        url = 'https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_laz'\n",
    "        url = '/'.join([url, file_path.name])\n",
    "        urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then setup the cluster that we will use for the computation using `dask`. For this example, the cluster consists of 2 processes (workers). Note: it is important that single-threaded workers are employed for the tasks that require `laserchicken`!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, \n",
    "                       n_workers=2, \n",
    "                       threads_per_worker=1, \n",
    "                       local_directory=tmp_folder/'dask-worker-space')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retiling\n",
    "\n",
    "The first step in the pipeline is to retile the retrieved point-cloud files to a regular grid, splitting the original data into smaller chuncks that are easier to handle for data processing. The boundaries of the grid and the number of tiles along each axis are set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'min_x': -113107.8100,\n",
    "    'max_x': 398892.1900,\n",
    "    'min_y': 214783.8700,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retiling of multiple input files consists of independent tasks, which are thus efficiently parallelized. The input controlling all the steps of the retiling is organized in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "retiling_out_path = tmp_folder/'retiled'\n",
    "\n",
    "retiling_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': tmp_folder.as_posix(),\n",
    "        'output_folder': retiling_out_path.as_posix()\n",
    "    },\n",
    "    'set_grid': grid,\n",
    "    'split_and_redistribute': {},\n",
    "    'validate': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retiling_macro = MacroPipeline()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    retiler = Retiler(input_file=file_path.name, label=file_path.stem)\n",
    "    retiler.config(retiling_input)\n",
    "    retiling_macro.add_task(retiler)\n",
    "\n",
    "retiling_macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "retiling_macro.run()\n",
    "retiling_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "Once the files are splitted into tiles of a manageable size, we proceed to the feature extraction stage, which is performed using `laserchicken`. We choose the following two example features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['mean_normalized_height', 'std_normalized_height']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base input dictionary for this step looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "dp_out_path = tmp_folder/'targets'\n",
    "\n",
    "dp_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': retiling_out_path.as_posix(),\n",
    "        'output_folder': dp_out_path.as_posix()\n",
    "    },\n",
    "    'load': {},\n",
    "    'normalize': {\n",
    "        'cell_size': 1\n",
    "    },\n",
    "    'generate_targets': {\n",
    "        'tile_mesh_size' : 10.0,\n",
    "        'validate' : True,\n",
    "        'validate_precision': 1.e-3,\n",
    "        **grid\n",
    "    },\n",
    "    'extract_features': {\n",
    "        'feature_names': feature_names,\n",
    "        'volume_type': 'cell',\n",
    "        'volume_size': 10\n",
    "    },\n",
    "    'export_targets': {},\n",
    "    'clear_cache': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `laserchicken` caches the KDTree computed for the point cloud. In order to free up the memory of the `dask` workers at the end of each tile's feature extraction, we need to clear the cache (see `clear_cache` in the input dictionary above).\n",
    "\n",
    "The tiles to which the original input file has been retiled are listed in a record file located in the retiling output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = []\n",
    "for file_path in file_paths:\n",
    "    record_file = '_'.join([file_path.stem, 'retile_record.js'])\n",
    "    with pathlib.Path(retiling_out_path/record_file).open() as f:\n",
    "        record = json.load(f)\n",
    "    assert record['validated']\n",
    "    tiles += [pathlib.Path(retiling_out_path/tile)\n",
    "              for tile in record['redistributed_to']]\n",
    "print([t.as_posix() for t in tiles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tile can be processed independently, so that again one can run the tasks in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_macro = MacroPipeline()\n",
    "\n",
    "for tile in tiles:\n",
    "    # parse tile index from the directory name\n",
    "    tile_index = [int(n) for n in tile.name.split('_')[1:]]\n",
    "    dp = DataProcessing(input=tile.name, label=tile.name, tile_index=tile_index)\n",
    "    dp.config(dp_input)\n",
    "    dp_macro.add_task(dp)\n",
    "    \n",
    "dp_macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "dp_macro.run()\n",
    "dp_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification of target points\n",
    "We can classify the target points according to their groud type, based on given cadaster data. \n",
    "To mark the types of the points in the point cloud, we can add a new column `ground_type` to the target point cloud. We can use the class code of TOP10NL as the identifier. \n",
    "\n",
    "0. Unclassified\n",
    "1. Gebouw\n",
    "2. Inrichtingselement\n",
    "3. Terrein (Polygon)\n",
    "4. Spoorbaandeel\n",
    "5. Waterdeel\n",
    "6. GeografischGebied (Point)\n",
    "7. FunctioneelGebied\n",
    "8. Plaats\n",
    "9. RegistratiefGebied\n",
    "10. Hoogte\n",
    "11. Relief (Line String)\n",
    "12. Wegdeel\n",
    "\n",
    "Here we present an example where we classify the points that fall on waterbodies with the given shp files of waterbody polygons. We will classify the target points according to the shape files provided in `testdata/shp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_path = pathlib.Path('./testdata/shp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline will automatically find out the relavant shp file. We will add a new column `ground_type`, and mark all points which fall in the waterbody polygons with `5`, which is the `waterdeel` identifier.\n",
    "\n",
    "We set up the input for the classification pipeline as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "cl_out_path = tmp_folder/'classified_target_point'\n",
    "\n",
    "classification_input = {\n",
    "    'setup_local_fs': {\n",
    "        'input_folder': dp_out_path.as_posix(),\n",
    "        'output_folder': cl_out_path.as_posix()\n",
    "    },\n",
    "    'locate_shp': {'shp_dir': shp_path.absolute().as_posix()},\n",
    "    'classification': {'ground_type': 5},\n",
    "    'export_point_cloud': {'overwrite':True}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we excute the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_macro = MacroPipeline()\n",
    "\n",
    "for tile in tiles:\n",
    "    tile_path = (dp_out_path/tile.name).with_suffix('.ply') \n",
    "    cl = Classification(input_file=tile_path.as_posix(), label=tile.name)\n",
    "    cl.config(classification_input)\n",
    "    cl_macro.add_task(cl)\n",
    "    \n",
    "cl_macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "cl_macro.run()\n",
    "cl_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GeoTIFF Export\n",
    "\n",
    "The last step of the pipeline is the transformation of the features extracted and added gound type from the point-cloud data and 'rasterized' in the target grid to a GeoTIFF file. In this case, the construction of the geotiffs (one per feature) can be performed in parallel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path where output will be written \n",
    "gw_out_path = tmp_folder/'geotiffs'\n",
    "\n",
    "gw_input = {\n",
    "    'setup_local_fs': {'input_folder': cl_out_path.as_posix(),\n",
    "                       'output_folder': gw_out_path.as_posix()},\n",
    "    'parse_point_cloud': {},\n",
    "    'data_split': [1, 1],\n",
    "    'create_subregion_geotiffs': {'output_handle': 'geotiff'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_macro = MacroPipeline()\n",
    "feature_names.append('ground_type')\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    gw = GeotiffWriter(bands=feature_name, label=feature_name)\n",
    "    gw.config(gw_input)\n",
    "    geotiff_macro.add_task(gw)\n",
    "\n",
    "geotiff_macro.setup_cluster(cluster=cluster)\n",
    "\n",
    "# run!\n",
    "geotiff_macro.run()\n",
    "geotiff_macro.print_outcome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stop the client and the scheduler of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
